{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 6: классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании вам предстоит построить классификатор текстов!\n",
    "\n",
    "Данные мы будем использовать из Kaggle соревнования: https://www.kaggle.com/competitions/nlp-getting-started/data Оттуда надо скачать файл train.csv. На обучающую и тестовую выборки его поделим кодом ниже, менять его не надо!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с датасетом постов из твиттера. Нам предстоит решать задачу бинарной классификации - определять содержатся ли в твитте информация о настоящей катастрофе/инциденте или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (0.5 балла)\n",
    "\n",
    "Выведете на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их пустой строкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 (1 балл)\n",
    "Давайте немного посмотрим на наши данные. Визуализируйте (где явно просят) или выведете информацию о следующем:\n",
    "\n",
    "1. Какое распределение классов в обучающей выборке?\n",
    "2. Посмотрите на колонку \"keyword\" - возьмите 10 наиболее встречающихся значений, постройте ступенчатую диаграмму распределения классов в зависимости от значения keyword, сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26481a7d970>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "i = data['keyword'].value_counts().index[1:11]\n",
    "v = data['keyword'].value_counts().values[1:11]\n",
    "plt.step(v, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3 (0.5 балла) \n",
    "\n",
    "В этом задании предлагается объединить все три текстовых столбца в один (просто сконкатенировать cтроки) и убрать столбец с индексом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>All residents asked to 'shelter in place' ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13,000 people receive #wildfires evacuation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Two giant cranes holding a bridge collapse i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Police investigating after an e-bike collide...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The Latest: More Homes Razed by Northern Cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1                    \n",
       "1         4                    \n",
       "2         5                    \n",
       "3         6                    \n",
       "4         7                    \n",
       "...     ...     ...      ...   \n",
       "7608  10869                    \n",
       "7609  10870                    \n",
       "7610  10871                    \n",
       "7611  10872                    \n",
       "7612  10873                    \n",
       "\n",
       "                                                   text  target  \n",
       "0       Our Deeds are the Reason of this #earthquake...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2       All residents asked to 'shelter in place' ar...       1  \n",
       "3       13,000 people receive #wildfires evacuation ...       1  \n",
       "4       Just got sent this photo from Ruby #Alaska a...       1  \n",
       "...                                                 ...     ...  \n",
       "7608    Two giant cranes holding a bridge collapse i...       1  \n",
       "7609    @aria_ahrary @TheTawniest The out of control...       1  \n",
       "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. h...       1  \n",
       "7611    Police investigating after an e-bike collide...       1  \n",
       "7612    The Latest: More Homes Razed by Northern Cal...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['keyword', 'location', 'text']\n",
    "data['text'] = data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'keyword', 'location'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4 (0.5 балла)\n",
    "\n",
    "Далее мы будем пока работать только с train частью.\n",
    "\n",
    "1. Предобработайте данные (train часть) с помощью CountVectorizer.\n",
    "2. Какого размера получилась матрица?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>bridge%20collapse  Ashes 2015: AustraliaÛªs c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>hail Carol Stream, Illinois GREAT MICHIGAN TEC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>police Houston  CNN: Tennessee movie theater s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>rioting  Still rioting in a couple of hours le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7445</th>\n",
       "      <td>wounds Lake Highlands Crack in the path where ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "1186  bridge%20collapse  Ashes 2015: AustraliaÛªs c...       0\n",
       "4071  hail Carol Stream, Illinois GREAT MICHIGAN TEC...       1\n",
       "5461  police Houston  CNN: Tennessee movie theater s...       1\n",
       "5787  rioting  Still rioting in a couple of hours le...       1\n",
       "7445  wounds Lake Highlands Crack in the path where ...       0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vec = CountVectorizer()\n",
    "X = cnt_vec.fit_transform(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x18455 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 86671 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5 (1 балл)\n",
    "\n",
    "В предыдущем пункте у вас должна была получиться достаточно большая матрица.\n",
    "Если вы взгляните на текст, то увидете, что там есть множество специальных символов, ссылок и прочего мусора.\n",
    "\n",
    "Давайте также посмотрим на словарь, который получился в результате построения CountVectorizer, его можно найти в поле vocabulary_ инстанса этого класса. Давайте напишем функцию, которая печает ответы на следующие вопросы:\n",
    "\n",
    "1. Найдите в этом словаре все слова, которые содержат цифры. Сколько таких слов нашлось?\n",
    "\n",
    "2. Найдите все слова, которые содержат символы пунктуации. Сколько таких слов нашлось? \n",
    "\n",
    "3. Сколько хэштегов (токен начинается на #) и упоминаний (токен начинается на @) осталось в словаре?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "t = str(cnt_vec.vocabulary_.keys())\n",
    "def func(X):\n",
    "    numb = len(re.findall('\\S*[0-9]\\S*', X))\n",
    "    punc = len(re.findall('\\S*[.,:;\\?!()]\\S*', X))\n",
    "    hesh = len(re.findall('[#@]\\S*', X))\n",
    "    return numb, punc, hesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3812, 18455, 0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6 (0.5 балла)\n",
    "\n",
    "Вспомним, что на семинаре по текстам мы узнали, что в nltk есть специальный токенизатор для текстов - TweetTokenizer. Попробуем применить CountVectorizer с этим токенизатором. Ответьте на все вопросы из предыдущего пункта для TweetTokenizer и сравните результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "# Чтобы узнать, какие параметры есть у этого токенайзера - используйте help(TweetTokenizer)\n",
    "# Для того, чтобы передать токенайзер в CountVectorizer используйте параметр tokenizer\n",
    "def tk(m):\n",
    "    f = TweetTokenizer()\n",
    "    return f.tokenize(m)\n",
    "cnt_vec = CountVectorizer(tokenizer = tk)\n",
    "X = cnt_vec.fit_transform(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x19670 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 94563 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "t = str(cnt_vec.vocabulary_.keys())\n",
    "def func(X):\n",
    "    numb = len(re.findall('\\S*[0-9]\\S*', X))\n",
    "    punc = len(re.findall('\\S*[.,:;\\?!()]\\S*', X))\n",
    "    hesh = len(re.findall('[#@]\\S*', X))\n",
    "    return numb, punc, hesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3946, 19672, 3155)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7 (2 балла)\n",
    "\n",
    "В scikit-learn мы можем оценивать процесс подсчета матрицы через CountVectorizer. У CountVectorizer, как и у других наследников \\_VectorizerMixin, есть аргумент tokenizer и preprocessor. preprocessor применится в самом начале к каждой строке вашего датасета, tokenizer же должен принять строку и вернуть токены.\n",
    "Давайте напишем кастомный токенайзер, которые сделает все, что нам нужно: \n",
    "\n",
    "0. Приведет все буквы к нижнему регистру\n",
    "1. Разобьет текст на токены с помощью TweetTokenizer из пакета nltk\n",
    "2. Удалит все токены содержащие не латинские буквы, кроме смайликов (будем считать ими токены содержащие только пунктуацию и, как минимум, одну скобочку) и хэштегов, которые после начальной # содержат только латинские буквы.\n",
    "3. Удалит все токены, которые перечислены в nltk.corpus.stopwords.words('english')\n",
    "4. Проведет стемминг с помощью SnowballStemmer\n",
    "\n",
    "Продемонстрируйте работу вашей функции на первых десяти текстах в обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1186    bridge%20collapse  Ashes 2015: AustraliaÛªs c...\n",
       "4071    hail Carol Stream, Illinois GREAT MICHIGAN TEC...\n",
       "5461    police Houston  CNN: Tennessee movie theater s...\n",
       "5787    rioting  Still rioting in a couple of hours le...\n",
       "7445    wounds Lake Highlands Crack in the path where ...\n",
       "151     airplane%20accident Somewhere Out There Expert...\n",
       "915     bloody Isolated City In World Perth 'I came to...\n",
       "1305    burning  @JohnsonTionne except idk them?? it's...\n",
       "2570    destroy he/him or she/her (ask) destroy the house\n",
       "7399    wounded Maracay y Nirgua, Venezuela Police Off...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Hello! ;) My name is Oleg #Russia You know?!?! :( I am sociable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(X):\n",
    "    X = X.lower()\n",
    "    X = TweetTokenizer().tokenize(X)\n",
    "    X = re.findall('#?[a-z]+', str(X))+re.findall('[.,:;\\?!]*[()]+', str(X))\n",
    "    noise = stopwords.words('english')\n",
    "    for i in noise:\n",
    "        if i in X:\n",
    "            X.remove(i)\n",
    "    stemmer = SnowballStemmer('english')      \n",
    "    X = [stemmer.stem(w) for w in X]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'name', 'oleg', '#russia', 'know', 'sociabl', ';)', ':(']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bridg',\n",
       " 'collaps',\n",
       " 'ash',\n",
       " 'australia',\n",
       " 'x',\n",
       " 'c',\n",
       " 'hail',\n",
       " 'carol',\n",
       " 'stream',\n",
       " 'illinoi',\n",
       " 'great',\n",
       " 'michigan',\n",
       " 'tec',\n",
       " 'polic',\n",
       " 'houston',\n",
       " 'cnn',\n",
       " 'tennesse',\n",
       " 'movi',\n",
       " 'theater',\n",
       " 's',\n",
       " 'riot',\n",
       " 'still',\n",
       " 'riot',\n",
       " 'coupl',\n",
       " 'hour',\n",
       " 'le',\n",
       " 'wound',\n",
       " 'lake',\n",
       " 'highland',\n",
       " 'crack',\n",
       " 'in',\n",
       " 'path',\n",
       " 'airplan',\n",
       " 'accid',\n",
       " 'somewher',\n",
       " 'expert',\n",
       " 'bloodi',\n",
       " 'isol',\n",
       " 'citi',\n",
       " 'in',\n",
       " 'world',\n",
       " 'perth',\n",
       " 'came',\n",
       " 'burn',\n",
       " 'johnsontionn',\n",
       " 'except',\n",
       " 'idk',\n",
       " 's',\n",
       " 'destroy',\n",
       " 'ask',\n",
       " 'destroy',\n",
       " 'the',\n",
       " 'hous',\n",
       " 'wound',\n",
       " 'maracay',\n",
       " 'nirgua',\n",
       " 'venezuela',\n",
       " 'polic',\n",
       " 'name',\n",
       " 'text',\n",
       " 'dtype',\n",
       " 'object',\n",
       " '(',\n",
       " ')']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token(str(train.text.head(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8 (1 балл)\n",
    "\n",
    "1. Примените CountVectorizer с реализованным выше токенизатором к обучающим и тестовым выборкам.\n",
    "2. Обучите LogisticRegression на полученных признаках.\n",
    "3. Посчитайте метрику f1-score на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7531064289573204\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "cnt_vec = CountVectorizer(tokenizer = token)\n",
    "X_tr = cnt_vec.fit_transform(train.text)\n",
    "X_ts = cnt_vec.transform(test.text)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_tr, train.target)\n",
    "pred = clf.predict(X_ts)\n",
    "print(f1_score(test.target, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 9 (1 балл)\n",
    "\n",
    "1. Повторите 7 задание, но с tf-idf векторизатором. Как изменилось качество?\n",
    "2. Мы можем еще сильнее уменьшить размер нашей матрицы, если отбросим значения df близкие к единице. Скорее всего такие слова не несут много информации о категории, так как встречаются достаточно часто. Ограничьте максимальный df в параметрах TfIdfVectorizer, поставьте верхнюю границу равную 0.9. Как изменился размер матрицы, как изменилось качество?\n",
    "3. Также мы можем уменьшить размер матрицы, удаляя слова со слишком маленьким df. Удалось ли добиться улучшения качества? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7450134770889488\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer(tokenizer = token)\n",
    "X_tr = tf_idf.fit_transform(train.text)\n",
    "X_ts = tf_idf.transform(test.text)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_tr, train.target)\n",
    "pred = clf.predict(X_ts)\n",
    "print(f1_score(test.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x16766 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 70396 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7450134770889488\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer(tokenizer = token, max_df = 0.9)\n",
    "X_tr = tf_idf.fit_transform(train.text)\n",
    "X_ts = tf_idf.transform(test.text)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_tr, train.target)\n",
    "pred = clf.predict(X_ts)\n",
    "print(f1_score(test.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x16766 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 70396 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 10 (1 балл)\n",
    "\n",
    "Еще один популяпный трюк, который позволит уменьшить количество признаков называется hashing trick. Его суть в том, то мы случайно группируем признаки ииии  ..... складываем их! А потом удаляем исходные признаки. В итоге все наши признаки это просто суммы исходных. Звучит странно, но это отлично работает. Давайте проверим этот трюк в нашем сеттинге.\n",
    "Также при таком подходе вам не нужно хранить словарь token->index, что тоже иногда полезно.\n",
    "\n",
    "1. Повторите задание 7 с HashingVectorizer, укажите количество фичей равное 5000.\n",
    "2. Какой из подходов показал самый высокий результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7499999999999999\n"
     ]
    }
   ],
   "source": [
    "hash_ = TfidfVectorizer(tokenizer = token, max_features = 5000)\n",
    "X_tr = hash_.fit_transform(train.text)\n",
    "X_ts = hash_.transform(test.text)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_tr, train.target)\n",
    "pred = clf.predict(X_ts)\n",
    "print(f1_score(test.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 58308 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 11 (1 балл)\n",
    "\n",
    "В этом задании нужно добиться f1 меры хотя в 0.75 на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7504096122337519\n"
     ]
    }
   ],
   "source": [
    "hash_ = TfidfVectorizer(tokenizer = token, max_features = 7000)\n",
    "X_tr = hash_.fit_transform(train.text)\n",
    "X_ts = hash_.transform(test.text)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_tr, train.target)\n",
    "pred = clf.predict(X_ts)\n",
    "print(f1_score(test.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
